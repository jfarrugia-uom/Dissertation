\chapter{Methodology}

%% \textbf{This section should include a recipe of what you did (explain what you have done so if someone wants to reproduce the experiment, they can).  A flow chart is typically helpful.  Also, make sure to define all software that you used including version numbers and OS.  Should also include a description of statistical methods used (if any).  \footnote{For more information see: \url{http://rc.rcjournal.com/content/49/10/1229.short}}}
We focus our hypernym discovery efforts on projection learning techniques.  In contrast to efforts undertaken to compare vector-combination supervision and unsupervised methods across the same corpus, datasets and evaluations metrics \citep{shwartz2017siege, roller2014inclusive, levy2015supervised}, the literature pertaining to \textit{projection learning} is fractured.  The experiments described in this section are motivated by two objectives.  

Firstly, we wanted to normalise the experimental conditions of the published methods such that they can be compared using the same methodology and metrics.  Some of the work claims that novelties introduced by the respective authors leads to improvement but the experiments were executed on a single fold of the dataset and no test results for statistical significance were provided \citep{Fu2014, ustalov2017negative, yamane2016distributional}.  In \citep{yamane2016distributional}'s case, the joint-learning mechanism was deemed to be superior to \citep{Fu2014}'s hard-clustering "pipeline" method but experiments on the English language were only executed on a single dataset.  Similarly, different metrics were used in different studies which makes it impossible to compare performance differences.  Lexical memorisation was acknowledged in \citep{espinosa2016supervised} but no study attempted to train and evaluate models on lexically-split datasets.  Other than \citep{espinosa2016supervised} who used sense-disambiguated vectors \citep{iacobacci2015sensembed}, all other studies employed word2vec embeddings.  We wanted to examine the performance of the models when a transformation matrix was learned on different pre-trained embeddings of the same dimensions.  

The second objective is to apply a similar experimental setup on the SemEval-2018 Task 9 challenge \citep{camacho2018semeval}.  This time, we focus on CRIM's soft-clustering algorithm \citep{bernier2018crim} inspired by \citep{yamane2016distributional}.  We attempt to introduce some observations made from the introductory round of experiments into the models we evaluate on the shared task data.  We also exploit the shared task's particularities (for instance terms are categorised as named entities or common concepts) to train two separate models on the two word categories and shared-layer models which have two different objectives but use the same feature extractor.  We train the models using three different embedding spaces, with vectors of larger dimension than attempted by CRIM \citep{bernier2018crim}.  Finally, we experiment with embeddings tuning motivated by \citep{howard2018universal} which yields results comparable (albeit inferior) to CRIM.  

%% Use this in evaluation:
%%The latter also tuned their embeddings to amplify their results but employed a different mechanism to ours. The paper's space constraints inhibited  \citeauthor{bernier2018crim} from describing how they tuned the embeddings.  Efforts to reproduce their method in our code yielded negative results.

\section{Hard-Clustering and Regularisation}
For this set of experiments, we leverage \citeauthor{ustalov2017negative}'s open-source code, publicly available on GitHub \footnote{https://github.com/nlpub/hyperstar}, the only fully-implemented projection learning model which was freely available at the time of that particular juncture in the project.  The authors built models featuring  negative sampling strategies expressed as regularisation terms (refer to section \ref{Ustalov}) and a basic implementation of \citep{Fu2014} excluding the segregation and separate training of direct and indirect hypernymy pairs which featured in the latter paper.

We extended the conditions of the original experiment in the following ways.  We trained and evaluated a model over five cross-validated folds and measured performance using the set of metrics provided by the SemEval-2018 Task 9 organisers \citep{camacho2018semeval}.  Additionally, we borrowed a routine from \citep{shwartz2016path} \footnote{https://github.com/vered1986/HypeNET/blob/v2/dataset/split\_dataset\_lexically.py} and cross-validated each model variant on a lexically-split dataset.  

We evaluated three models which included the original \citep{Fu2014} model and \citeauthor{ustalov2017negative}'s best performing variants featuring re-projected asymmetric and neighbour regularisation.  To measure the impact of regularisation on cluster size, we clustered our training set fold and learned piece-wise transformation matrices on 1, 10 and 25 clusters and repeated the process for each model strategy.  In their original work, \citeauthor{ustalov2017negative} produced results indicating improvements yielded by their regularised models on \citep{Fu2014}'s baseline but did not report statistical significance.  By cross-validating the published models we obtained a distribution of scores which allowed us to compute a two-sample $t-$test and measure whether the mean \ac{MAP} returned by a negatively-sampled model variant is significantly better than a baseline.  

All models were trained and cross-validated on the combined English dataset employed in \citep{ustalov2017negative} consisting of the amalgam of \textit{BLESS} \citep{Baroni2011}, \textit{EVALution} \citep{santus2015evalution}, \textit{ROOT09} \citep{santus2016nine}, and \textit{K\&H+N} \citep{necsulescu2015reading}.  The original code was composed of 4 separate scripts, intended to be called from the shell, responsible for: extracting the embeddings for the words in the dataset; training a $k-$means clustering model (for a given $k$); training $k$ projection matrices; and evaluating the results based on \citeauthor{ustalov2017negative}'s chosen metrics.  We modified the original scripts, converting each independent script into simple APIs which were then invoked from a central Jupyter notebook test harness.

The dataset was split into 5 folds, with 80\% of the word-pairs dedicated to training and the remaining 20\% serving as validation gold data.  Prior to splitting the data, we eliminated words which had no corresponding vector in the respective embeddings space, dropping around 20 pairs.  The random-split ensured that a query term (and corresponding hypernyms) featured in either the training set or test set but never in both.  However, high frequency hypernyms feature in both training and test sets, albeit associated with different hypoynm words.  To avoid hypernyms appearing in both sets (and consequently, the model learning a transformation that projected words to prototypical hypernyms), a lexical-split was also carried out.

We followed the paper, by optimising the MSE loss function with Adam \citep{kingma2014adam} using the default parameters, and we trained each model for 700 epochs on batches of 1,024 training pairs. The projection matrix was initialised with small numbers drawn from a random normal distribution with $\mu=0$ and $\sigma=0.01$.  For the regularised, negatively-sampled models, we set the constant $\lambda$ to 1, which returned the best results when applied to the re-projected regularisation models in the original paper.  Each vector in the embeddings space was normalised to unit-norm before training.  This allows a simple dot-product operation to determine the similarity between words in the embeddings space and the projected hypernym.

We modified the \citeauthor{ustalov2017negative}'s evaluation routine, by finding the words in the vector space which were most similar to the projected hypernyms.  The latter were generated by combining the hyponym embeddings to the learned projection matrix by dot product which yields a new 300-dimensional vector: the term's hypernymy estimate.  We chose the optimal transformation matrix from the $k$ projections learned, by first predicting the cluster to which an unseen word-pair belongs based on the vector offset of the hypernym and hypoynm in that pair.  A ranked list of the 15 most similar words (in order of descending similarity) to the term's hypernym projection vector is associated with every query term in the test fold.  The predictions were finally evaluated against the real hypernyms in the test fold by invoking the shared task's scorer \footnote{https://competitions.codalab.org/competitions/17119\#learn\_the\_details-evaluation-details}, slightly modified to accept a prediction Python dictionary rather than tab-separated file.  

To determine whether the projection models were superior to na\"ve methods, we created two baselines inspired by the literatue.  One is a simple unsupervised model similar to the \citep{maldonado2018adapt} shared task submission, which selects the 15 embeddings closest to query term vector from the embeddings space.  The other, also employed as a baseline in the shared task \citep{camacho2018semeval}, emits the 15 most frequent hypernyms from each training fold, irrespective of the term in the validation fold.  

\subsection{Alternative Embeddings}
\begin{table*}\centering
    \begin{tabular}{@{}llrrr@{}} \toprule
    & \textbf{Data} & \textbf{Dimension} & \textbf{Vocab Size} & \textbf{Tokens} \\ \cmidrule{2-5}
    \textbf{word2vec} & Google News & 300 & $\approx$3M & 100B \\ \midrule
    \textbf{GloVe} & Common Crawl & 300 & $\approx$1.9M & 42B \\ \midrule
    \textbf{fastText} & \shortstack[l]{Wikipedia 2017\\UMBC\\statmt.org News} & 300 & $\approx$1M & 6B \\
    \bottomrule
    \end{tabular}
    \caption{Basic details of embeddings used in own experiments.}\label{tab:experiment_embeddings}
\end{table*}

\citeauthor{ustalov2017negative} reported results obtained using word2vec embeddings pre-trained on the Google News corpus \citep{mikolov2013efficient}.  Alongside the word2vec embeddings, we train and validate the models using feature vectors from GloVe \citep{pennington2014glove} and fastText \citep{bojanowski2017enriching} pre-trained on large corpora, which were available online.  The basic details of the pre-trained embeddings we used are illustrated in Table~\ref{tab:experiment_embeddings}.

\subsection{Computational Setup}
The experiments were executed within a Jupyter Notebook Docker container \footnote{https://hub.docker.com/u/jupyter/}, pre-loaded with the Anaconda Python 3.6 distribution and including the scientific Python modules.  We installed the latest stable version of TensorFlow \citep{tensorflow2015-whitepaper} and Gensim \citep{rehurek_lrec} as well as other packages specific to the \citep{ustalov2017negative} setup.  The guest Docker operating system was Ubuntu Linux 18.04 LTS which was hosted on a \textit{t2.xlarge} AWS EC2 instance \footnote{https://aws.amazon.com/} which ran on Amazon Linux 2.  The original model used earlier versions of all dependencies.  Before proceeding with our own experiments with this setup we executed the original code with the parameters specified in the paper and ensured the generated and published results matched.


%% mention AWS EC2 setup
%% jupyter-scipy docker, based on a Python 3.6 environment to avoid installing dependencies
%% Upgraded dependencies from Ustalov's original experiment.  