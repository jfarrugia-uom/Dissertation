%%\textbf{Note that you may have multiple \texttt{{\textbackslash}include} statements here, e.g.\ one for each subsection.}\cofeBm{0.7}{1}{0}{3cm}{-1cm}
\chapter{Introduction}

\section{Motivation}
Humans conceptualise objects as part of classes.  For instance, a dog is a type of animal. This information
enables one to think of objects as abstract classes, which means that if a human learns that dogs bite, it might infer that other animals might bite too.  This type of knowledge inference is important to generic Artificial Intelligence. In this research, the plan is to focus on identifying the general class that a concept or entity belongs to — linguistically, this is referred
to as the hypernym.  Hypernym discovery has been a challenging task in Natural Language Processing, reflected by a number of SemEval shared tasks set in the past, which mainly focused on Taxonomy evaluation (SemEval-2015 task 17\footnote{\url{http://alt.qcri.org/semeval2015/task17/}}, SemEval-2016 task 13\footnote{\url{http://alt.qcri.org/semeval2016/task13/}}).  

\citeauthor{camacho2017we} muses about the recent neglect of taxonomy construction in research, in favour of ``simply'' identifying hypernymy \citep{camacho2017we}.  He blames this primarily on the reliance on hand-crafted taxonomies for the evaluation of proposed solutions, which are expensive to procure.  On the other hand, hypernym detection is appreciably easier to evaluate due to the availability of several gold-standard datasets \citep{Baroni2011, santus2015evalution, weeds2014learning}.  Despite several developments in the area of hypernym detection, Camacho-Collados argues that merely determining whether hypernymy holds between a word-pair in a binary fashion is of limited use to downstream tasks \citep{camacho2017we}.  Consider the question “Which museums are open on Sunday in London” posed to a Question Answering system.  An effective system must be able to autonomously link various building instances with the \textit{museum} concept, before filtering those that are located in London and open on a Sunday.  Thus, the binary task is recast to a hypernym discovery problem whereby given a query term, a system is expected to emit a list of the word’s potential hypernyms.  Question Answering is not the only domain which benefits from a hypernym discovery component.  Other downstream tasks include taxonomy construction \citep{wu2012probase}; and query understanding \citep{hua2017understand}.

The call to recast identification to discovery was answered by a recent SemEval committee who devised a shared task - SemEval 2018 Task 9 \footnote{\url{https://competitions.codalab.org/competitions/17119}} - that challenged participants to extract hypernyms directly from general-purpose and domain-specific corpora in English, Spanish and Italian.  We decided to focus on English, the language which enjoys the highest literature coverage and for which several resources are widely available.  Previous research in hypernym identification and discovery was often dependent on highly-curated datasets and Wikipedia as source corpus \citep{Snow2004, shwartz2017siege, levy2015supervised, weeds2014learning}.  The Shared Task deviates from the norm on both counts.  The organisers used the 3-billion-word \textit{UMBC} corpus as the English general-purpose corpus containing high quality paragraphs sourced from the web, and is more diverse than Wikipedia.  The datasets composed of training, test and validation gold standards and a 220K-word vocabulary were entirely collected from the corpus.  The hypernyms were initially derived from linguistic resources and thereafter validated through crowdsourcing and expert verification.  However, the quality across the dataset is uneven, and several gold standard hypernyms we encountered were not entirely accurate.  Subsequently, the datasets are not over-engineered and have a ``real life'' quality to them which provide an additional challenge to hypernym discovery solutions.  We make extensive use of the resources shipped with the Shared Task, and we also study techniques from the best submission's toolbox to fuel our exploration of hypernymy discovery.

\section{Aims \& Objectives}
Research on hypernym identification, extraction and discovery has been ongoing for more than 25 years.  As a consequence, a staggering amount of techniques have been developed which attempt to solve some aspect of the problem.  Despite the time and effort, hypernym discovery has not been solved yet.  However, a recent development in supervised machine learning algorithms known as \textit{projection learning} models have shown ability at generating hypernyms for a given query term.  These models exploit the  linguistic regularities preserved in word embeddings vector spaces to estimate a linear transformation matrix which when applied to a query vector, projects it close to the query's hypernym vector.

However, the research is currently fractured insofar that no work has attempted to unite all projection models in the literature and examine them with respect to a common experimental regime.  Moreover, we found that claims by authors attesting their model's superiority over other work to be statistically weak. Finally, the majority of the work focuses on word features extracted from word2vec embeddings thus neglecting the effect that other embeddings algorithms such as GloVe and fastText might have on projection learning models' performance.

Our aim is to develop a deep understanding of projection learning such that we can appraise the various solutions critically.  We plan to apply a rigorous methodology to quantify the differences, if any, between four of the projection learning models we reviewed in the literature.  The insight will then be applied on the SemEval-2018, Task 9 shared task, specifically the English, general-purpose sub-task.  To fulfil these aims, we set out the following objectives:
\begin{enumerate}
    \item Review the prevalent literature on projection learning methods to understand the mathematical background which underpins these models;
    \item Implement one or more of the models as per the relevant technical paper;
    \item Explore the effect of word2vec, GloVe and fastText word features on the models;
    \item Train our embeddings on the given corpus;
    \item Apply sophisticated machine learning techniques such as dropout, early stopping, multi-task learning and transfer learning;
    \item Analyse the models' performance within a statistical framework;
    \item Test our knowledge of a hypernym discovery method, by applying our best model on the Shared Task challenge;
\end{enumerate}

\section{Project Outline} % Perhaps call it Project Outline since my work is more exploratory in nature
We selected the following four projection learning methods which were hitherto evaluated on different datasets and metrics, and  therefore could not be directly compared with each other:
\begin{enumerate}
    \item The original method developed by \citet{Fu2014} which first partitioned the dataset into $k$ clusters using the $k-$means unsupervised algorithm fitted on the hyponym-hypernym vector offsets, and subsequently learned $k$ piecewise linear, projection transformations.  The error between the estimated, projected hypernym and gold-standard hypernym is measured using the \ac{MSE} objective function;
    \item An extension of \citep{Fu2014} which introduces negative samples as regularisation terms \citep{ustalov2017negative};
    \item A joint-learning approach proposed by \citet{yamane2016distributional}, which clusters the data samples and learns the projection matrices in a single training phase.  It deviates from the preceding methods by using the dot-product operation to measure the similarity between the predicted and actual hypernym.  This method casts hypernym discovery as a hypernym identification problem and employs the binary cross-entropy objective function to classify a query and a candidate word as hypernymy;
    \item An extension to \citet{yamane2016distributional} which learns multiple projections of a query vector, combining each projection with the actual hypernym using the dot-product operation to obtain a similarity score of each projection and passing the linear combination of similarities to a sigmoid activation layer \citep{bernier2018crim}.  Similar to \citet{yamane2016distributional}, the error is measured via the binary cross-entropy function.  This method, dubbed CRIM, was the highest-ranking submission in all English sub-tasks.
\end{enumerate}
We emulated work such as \citep{shwartz2017siege, levy2015supervised}, cross-validated each model on a combination of four commonly-used datasets \citep{santus2015evalution, Baroni2011, santus2016nine, necsulescu2015reading}, on the same metrics favoured by the Shared Task organisers \citep{camacho2018semeval}.  To our knowledge, we are the first to have analysed these four projection learning methods under the same experimental conditions and with features extracted from word2vec, GloVe and fastText.

We developed our version of the binary cross-entropy models and training algorithms but modified the published \ac{MSE} models developed by \citeauthor{ustalov2017negative} to use our dataset, evaluation metrics and scoring system.  We found that the performance of our version of CRIM is significantly better than the \ac{MSE} models and faster to train than the joint-model proposed by \citet{yamane2016distributional}.  We also discovered that fastText features also contributed to significantly better scores, with respect to our chosen dataset, metrics and methodology.

In the original CRIM, \citet{bernier2018crim} tune the embeddings whilst learning the projections and logistic regression coefficients.  To avoid obliterating the embeddings' word information through large gradient updates during the first training cycles, the author reduced the learning rate by an order of magnitude and required several hundred cycles to achieve convergence \footnote{These details were omitted from the technical paper but were discussed in private correspondence with Gabriel Bernier-Colborne, the paper's main author.  Typical hyperparameter values were provided as defaults in \url{https://github.com/gbcolborne/hypernym_discovery/blob/master/hparams.conf}}.  We opted for a different strategy, training the model in two phases: first we kept the embeddings frozen and learned the projections and logistic regression coefficients.  Once we trained the best model we could, we froze the projection layer weights and tuned the embeddings' weights.  This allowed us to reduce the training cycles drastically: a maximum of 15 epochs in the first phase; and up to 3 epochs in the tuning phase.  

We obtained interesting results when we applied our model on the Shared Task English dataset. Our CRIM version, trained on untuned fastText embeddings eclipsed the scores reported by \citet{bernier2018crim} when they trained their model with frozen embeddings.  Despite this promising result, we did not manage to out-rank the original CRIM in the Shared Task leader-board.  Our best solution which scored a \ac{MAP} and \ac{MRR} of 0.173 and 0.348 respectively, ranked overall third.
\citeauthor{bernier2018crim} also ranked second with a slight variation on their original model which obtained a \ac{MAP} of 0.195 and \ac{MRR} of 0.360.  The top scores were only marginally higher (0.198 and 0.361).

\section{Document Structure}
We present the background required to understand this task, and the experiments we carried out, by surveying the salient contributions made to the areas of hypernymy identification and discovery.   We start from the earliest handpicked pattern-based methods and see how even this relatively simple technique can be leveraged to induce a large, complex taxonomy.  

Dependency paths were later used to discover hypernym-containing patterns automatically.  Although never abandoned, pattern-centric approaches eventually gave way to distributional methods and \acl{VSM}s.  There are several \ac{VSM} flavours, each varying in terms of context type and feature weighting mechanisms; we will introduce the main types used in the reviewed literature.  Several unsupervised metrics were developed that, given the vector representation of two words, scored the likelihood that the words were bound by hypernymy based on their respective dimensional contexts.

When word embeddings burst onto the scene in 2013 \citep{mikolov2013distributed}, supervised methods involving various combinations of the hyponym and hypernym vector embeddings, fed into a classifier acquired outstanding results in the hypernym identification binary task.  Closer inspection by researchers, sceptical about these results, underscored these methods’ tendency to overfit the training data \citep{levy2015supervised, santus2016nine}. In doing so, they shone a light on the limitation of the identification task and encouraged the NLP research community to recast identification to hypernym discovery.  We then move on to a variant of supervised learning - projection learning - which attempts to learn a linear projection transformation matrix that, when applied to a hyponym vector, yields a vector close to its hypernym.  This research project is particularly focused on these methods, considering their relative success in generating hypernyms.  

We close the background chapter by reviewing the SemEval 2018 Task 9 Shared Task \citep{camacho2018semeval}, concentrating on the methodology used to create the training and testing datasets which were partly automated and partly crowd-sourced.  Setting aside the popular precision/recall/\(F_1\) evaluation measures, the task’s organisers propose a new set of metrics, borrowed from \ac{IR} that are suited to the ranking nature of the problem.  Finally, we examine the submitted models focusing on those which peruse of projection learning techniques to propose hypernyms for the given candidate terms.

In Chapter 3, we describe the method adopted to run two sets of experiments.  In the first, we perform a statistical analyses of four projection learning models, experimenting with various hyperparameter settings and word embeddings.  Considering that any performance differences coming from the various configurations could be due to one or several parameters (or combinations of), we conduct $n$-way \ac{ANOVA} analyses on the results and, where applicable, run post-hoc analyses to explore which factor levels were responsible for the significant result changes.  For these experiment we used publicly available, standard pre-trained embeddings.

In the second round of experiments, we learn custom embeddings on the Shared Task's 3-billion \textit{UMBC} corpus, such that we can train our model on word2vec, GloVe and fastText word features.  We focus on our implementation of the CRIM algorithm, which performs well and is easy to train, testing various model/training configurations including a multi-task learning setup and transfer learning method which trains projections and embeddings in two distinct phases.

All artefacts were developed with Python 3.6, making use of scientific libraries such as \texttt{numpy}, \texttt{scipy} and \texttt{statsmodel}.  We built our models using \texttt{Keras}, an open-source machine learning framework, which abstracts away complex tensor operations and allows the models to be built modularly \citep{chollet2015keras}.  We ran our experiments in Jupyter Notebook, an interactive environment which encourages agile iterations and code sharing \citep{kluyver2016jupyter}.  Embeddings were trained using the native \texttt{C++} implementations and then loaded into \texttt{KeyedVectors} with Gensim \citep{rehurek_lrec}.

We present our results in Chapter 4 using a mix of tabular data and plots which were rendered using Python's \texttt{matplotlib} and \texttt{seaborn} libraries.  Prior to presenting the experiments' results, we describe the datasets used in terms of their descriptive statistics.  The statistical evaluation of the projection learning models we analysed is presented in Chapter 5.  We also investigate the relationship between high frequency hypernyms and model performance, a phenomenon referred to as \textit{lexical memorisation} \citep{levy2015supervised}, towards which the early supervised models were especially prone.  The results our model obtained on the Shared Task are comparable with scores yielded by the official submissions, and the baselines furnished by the organisers.  We did not perform  rigorous statistical analyses of our results because we do not have sufficient data with which to compare our artefacts.

Concluding remarks, lessons learnt and new directions in which this research question can be taken are shared in Chapter 6.
