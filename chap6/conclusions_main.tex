%\textbf{This section should have a summary of the whole project.  The original aims and objective and whether these have been met should be discussed. It should include a section with a critique and a list of limitations of your proposed solutions.  Future work should be described, and this should not be marginal or silly (e.g.\ add machine learning models).  It is always good to end on a positive note (i.e.\ `Final Remarks').}
\chapter{Conclusions}
In this dissertation we tackled hypernym discovery, a topic in computational semantics which focuses on algorithms which can suggest the hypernyms of given query words.  Hypernym discovery is an evolution of the hypernym identification binary task, in which a model is expected to detect whether two words are related by hypernymy.  Pattern-based and distributional semantic models have been proposed as solutions to either task.  We focused on \textit{projection learning}, a supervised distributional method which learns a transformation matrix that when combined with a hyponym's embeddings generates an estimate of its hypernym vector.  Although originally developed as an alternative hypernym identification solution, it was later found to be well-suited for hypernym discovery.  We were further motivated to explore these algorithms after a  projection-learning-based model, submitted by the Computer Research Institute of Montreal, achieved the highest rank in all English subtasks in the SemEval-2018 Task 9 challenge.

\section{Achieved Aims and Objectives}
We divided our empirical work in two sections.  We first reviewed four projection learning methods and found two overlooked gaps in the literature.  To our knowledge, there was no published comparative analysis of all four models using a common dataset and evaluation metrics, which made it impossible to determine whether one or more models had an edge over the others.  Secondly, word2vec features were exclusively used to train all four models when applied to English-language hypernym discovery tasks.

Inspired by \citet{shwartz2017siege} work with unsupervised methods, we devised an experimental setup against which to test the chosen four projection model variants.  

\begin{table*}\centering
\begin{tabular}{@{}llll@{}}\toprule
\textbf{Model} & \textbf{Loss Function} & \textbf{Hyperparameter} & \textbf{Values}\\ \midrule
\multirow{2}{*}{\citep{Fu2014}} & \multirow{2}{*}{\ac{MSE}} & Embeddings & w2v, GloVe, ft\\
&& Clusters & 1, 10, 25\\ \midrule
\multirow{3}{*}{\citep{ustalov2017negative}} & \multirow{3}{*}{\ac{MSE}} & Embeddings & w2v, GloVe, ft\\
&& Clusters & 1, 10, 25\\
&& Regularisation & Asym., Neigh.\\ \midrule
\citep{yamane2016distributional} & Log-loss & Embeddings & w2v, GloVe, ft\\ \midrule
\multirow{4}{*}{\shortstack[l]{(Bernier-Colborne, and\\Barri√®re, 2018)}} & \multirow{4}{*}{Log-loss} & Embeddings & w2v, GloVe, ft\\
&& Projections & 1, 5, 10\\
&& Negative Samples & 1, 5, 10\\
&& $\lambda$ & 0, 0.1, 1\\
\bottomrule
\end{tabular}
\caption{Summary of tested factors in comparative analysis.}\label{tab:conc_experiment_summary}
\end{table*}

We borrowed the English Combined Dataset from \citet{ustalov2017negative} consisting of four high-quality datasets and partitioned it into $k=5$ folds.  We cross-validated each model using \ac{MRR}, \ac{MAP} and P$@k$ metrics mandated by \citet{camacho2018semeval} in the Shared Task.  We tested several versions of every model, each initialised with a different set of hyperparameters, chosen based on their claimed effect on performance in the respective publications.  We treated the embeddings like another hyperparameter, fitting each model with features extracted from publicly available, pre-trained embeddings created using word2vec (skip-gram, negative sampling), GloVe and fastText.  Each experiment yielded a distribution of scores which allowed us to perform multi-factor statistical tests using the \ac{ANOVA} framework.  Our salient findings are summarised below:
\begin{enumerate}
    \item The choice of embeddings has a significant effect on both \ac{MSE} and log-loss models.  fastText was significantly better than GloVe and word2vec in almost all of the tested configurations;
    \item GloVe embeddings are statistically a poor choice for log-loss models;
    \item Contrary to \citet{ustalov2017negative} findings, regularisation did not have a significant effect on English hypernym discovery, irrespective of the embeddings and number of clusters learned.  We confirmed that the scores increased when using regularisation in a single-cluster, word2vec context, but the improvement was not significant;
    \item Learning several projections in the multi-projection model has a significantly positive effect but is dependent on choice of embeddings;
    \item Log-loss models are significantly superior to \ac{MSE} models, while no difference was detected between the joint Yamane model and multi-projection CRIM model.  However, CRIM converges and generates hypernym predictions faster than Yamane.
\end{enumerate}

The \ac{MSE} models were published on GitHub by \citeauthor{ustalov2017negative}, and we modified the code to fit our experimental requirements.  Following the technical papers, we built the log-loss (binary cross-entropy) neural models and the training algorithms from scratch using TensorFlow's \texttt{Keras} machine learning framework.  We extended CRIM by integrating both asymmetric and neighbour regularisation terms from \citet{ustalov2017negative} training the model on both random and semantically-related negative samples. 

To predict hypernymy, the binary cross-entropy models require the query word as well as candidate hypernym which results in testing every vocabulary word (consisting of around 200K words) against each query term and selecting the words returning the highest hypernymy probability.  To accelerate this process we extracted the weights from the models and customised the forward-pass using fast \texttt{numpy} operations, significantly reducing the time required to generate the test dataset's query predictions.  

By addressing the aforementioned gaps and developing two models from scratch, we were able to develop a deep understanding of the models, which was the initial aim of this dissertation.  We showed that the multi-projection model developed by \citet{bernier2018crim} was statistically superior to the \ac{MSE} models that preceded it and faster to train than the original dot-product based model which inspired it \citep{yamane2016distributional}.  

We applied the multi-projection (CRIM) model to our second set of experiments: the English, general-purpose subtask outlined in the Shared Task.  For this task we trained word2vec, fastText and GloVe 300-dimensional embeddings on the 3-billion-word \textit{UMBC} corpus.  With frozen fastText embeddings and using no external resources, our model achieved \textbf{0.267} \ac{MRR} and \textbf{0.133} \ac{MAP}, a substantial improvement on the original CRIM which managed \textbf{0.172} \ac{MRR} and \textbf{0.080} \ac{MAP} when trained on frozen word2vec embeddings.  We also underline that the model converged in under 15 epochs, requiring less than 5 minutes training time.  The model would have ranked third overall in the Shared Task's general-purpose English subtask leader-board, preceded only by the \ac{CRIM} group, who submitted two runs.

Furthermore, we applied two sophisticated machine-learning techniques to the vanilla model.  We implemented a multi-task setup which trained two separate classifiers for concepts and entities whilst sharing the same feature extractor.  Despite the additional complexity, this had a negligible effect on the score, actually reducing performance on fastText vectors by a slight margin when compared to the vanilla setup.  We also tuned the embeddings, adopting a different approach to \citeauthor{bernier2018crim}.  Instead of training embedding and projections in the same phase, we split training into two phases.  We froze embeddings and trained projections in the first phase and froze the projections and tuned the embeddings in the second phase.  With only an additional 2 epochs of training, the strategy boosted our fastText \ac{MRR} and \ac{MAP} metrics by 30.5\% to 0.348 and 0.173 respectively.  Despite the improvement, we were not able to beat the original CRIM top-ranked submission which scored 0.361 \ac{MRR} and 0.198 \ac{MAP}.  However, to train embeddings and projections in one phase, the authors had to reduce the learning rate by an order of magnitude, clip the gradients and subsequently train their model for hundreds of epochs which is more expensive than our approach.

\section{Critique and Limitations}
The performance of projection learning methods seem to largely influenced by the choice of dataset.  This can be seen from the sharp decline in performance when evaluating our models on the Shared Task's more diverse dataset.  The more we move away from strict taxonomic definitions, the harder recognising hypernymy becomes.  This was observed on the Combined Dataset where 31\% of the word-pairs were related to the natural world.  Although there were significant fluctuations, all models performed relatively well on this task.  However, they do not possess the sophistication to discern among similar hypernyms.  For instance, the transformation matrix will project the hypernym for the term \textit{cat} in the vicinity of the \textit{animal} hypernym vector. But the model would also retrieve false positive candidates like \textit{invertebrate}, which are related to \textit{animal} but unrelated to \textit{cat}.

Supervised models are only as good as their training datasets.  We should keep in mind that there is an element of subjectivity even in the way humans perceive hypernymy.  This was evident in the Shared Task dataset which contained noisy training tuples, although it not clear how susceptible the models were to these samples.  In both Combined and Shared Task datasets, there was an observable tendency to project terms close to high frequency hypernyms seen during training.  Conversely, rarer hypernyms were unlikely to be discovered.  In Chapter 5, we plotted the relationship between test hypernym occurrence frequency in the Combined Data training set and average precision score per term.  There is a positive correlation between the two variables so we cannot rule out lexical memorisation.  However, we do not think the phenomenon is as pronounced as in simple binary classifiers trained to identity hypernymy; the projection models are also capable of surprising us by correctly retrieving relative rare hypernyms pertaining to non-trivial terms.

We achieved our best results when we tuned the embeddings.  However, we dwell on whether this strategy was merely a short-cut to amplify the scores and wonder whether the model ability to "understand" hypernymy really benefited from this.  Although we cannot argue with higher scores, we also noticed an increased predisposition towards projected the most frequent hypernym in the set.  The most galling example is the hypernym \textit{person} which was incorrectly predicted as a first-choice hypernym for nearly 350 test terms after tuning, up from 85 terms before tuning.

In this dissertation, we largely ignored the challenges of homonymy and polysemy. In the shared dataset, the organisers simplified the problem for us by conflating word senses into a single lexicalisation.   The vector representation of a polysemous word will arguably be a reflection of the context of that word in the corpus.  Word embeddings rely on latent dimensions which are not directly interpretable so we do not know exactly which sense/s of the word have been captured by the embeddings.  Finding the closest words to a polysemous in the embeddings space by cosine similarity may give some indication of how the word was "distilled" by the embeddings.  For instance in our word2vec embeddings trained on the \textit{UMBC} shared task corpus, the 15 most similar words to \textit{shell} are all related to the command-line interface sense of the work.  The projection learning methods show some ability at capturing the hypernyms for one sense of the word or another, but not likely to capture all word senses.  Specialised embeddings like \textsc{SenseEmbed} \citep{iacobacci2015sensembed} learn word embeddings for BabelNet \citep{navigli2012babelnet} synsets but require the corpus to be fully sense-disambiguated first.

Finally, projection learning systems have been used to discover hypernyms in languages other than English.  We reviewed work which describes their deployment on Chinese, Russian and Japanese.  However, we do not know how our favoured multi-projection model reacts to languages other than English.  

\section{Future Work}
Supervised methods generally benefit from larger training sets.  One way of augmenting the dataset is by using external lexical resources such as BabelNet.  BabelNet exposes a REST API which would allow us to search for a query term's co-hyponyms.  By definition co-hyponyms share the same hypernyms which we already have in our dataset.  We could then create new training examples by linking the co-hyponyms we retrieve from Babelet and the corresponding hypernyms from our given dataset.  Having a larger dataset would allow us to downsample highly frequent hypernyms in the dataset, much like embeddings training algorithm allow the subsampling of high frequency, less informative words.  Both dataset augmentation and subsampling were used in \cite{bernier2018crim} with mixed results.

Projection learning techniques depend on embeddings for word features.  However, the objective functions of the embeddings algorithms we chose are not designed with the hypernymy semantic relationship in mind.  In fact a term's most similar words in the word2vec, GloVe and fastText spaces tend to be synonyms or co-hyponyms.  Specialised embeddings which specifically learn the hypernym semantic relations have been developed.  \textit{HyperVec} is one example whereby the authors propose two objective functions to: train hypernym embeddings to have a higher Euclidean norm than hyponyms; to instil asymmetry in the embeddings such that the cosine similarity metrics is sensitive to word order \citep{nguyen2017hierarchical}.  \textit{HyperVec} embeddings can be used in an unsupervised context but whether they can be effectively used to train a projection learning model remains to be seen.

Arguably, one of the greatest challenges in hypernym discovery is the application of the techniques in a multi-lingual setting, especially for low-resource languages such as Maltese.  Applying effectively a projection learning approach would require high-quality embeddings which in turn depend on a corpus, large and diverse enough to somewhat encapsulate this relationship in a distributional manner.  Furthermore, a sufficiently large gold-standard dataaset would be required to train a projection learning model (or any other supervised model) as well as to evaluate the result.

The methodology employed by the SemEval 2018, Task 9 organisers which leveraged crowsdsourcing to verify semi-autonomously collected hypernyms works given a decent corpus and a rich, multilingual lexical resource such as BabelNet.  BabelNet already covers 2.5 million Maltese synsets and 5.1 millions word senses\footnote{Stats acquired from \url{https://babelnet.org/stats}} but we are not sure to what extent the synsets are linked semantically, especially considering that only 4,636 definition have been extracted from Maltese resources.  Possibly, a small evaluation Maltese gold-standard set can be created by translating an existing dataset such as the Combined Dataset we used in our own experiments.

In the absence of training data in a particular language, promising work has been done in the area of cross-lingual distributional semantics which leverage well-resourced languages to produce word representations for related, under-resourced languages.  In their recent paper, \citet{upadhyay2018robust} proposed an unsupervised method which learns bilingual, dependency-based word embeddings for a low-resource language by using a delexicalised parser trained on a treebank of a similar language.  There method was so far validated on a simulated low-resource language, whereby the resources were depleted manually.  Evaluating this method on a real under-resourced language like Maltese would be an interesting avenue of research to explore. 